{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4-1](./img/4-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4-2](./img/4-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4-3](./img/4-3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed backprop:  [-1.  9.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([1.0,2.0,3.0])\n",
    "w=np.array([1.0,2.0,3.0])\n",
    "y_hat=x*w\n",
    "y=np.array([2.0,4.0,6.0])\n",
    "s=y_hat-y\n",
    "loss=np.square(s)\n",
    "rloss_rw=np.diff(loss)/np.diff(s)*np.diff(s)/np.diff(y_hat)*\\\n",
    "            np.diff(y_hat)/np.diff(w)\n",
    "print(\"Computed backprop: \",rloss_rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4-4](./img/4-4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "w1=2\n",
    "w2=2\n",
    "b=1\n",
    "x=np.array([1.0,2.0,3.0])\n",
    "x_2=x**2\n",
    "x_data=np.vstack([x,x_2])\n",
    "y_data=x_2*w2+x*w1+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 21.0\n",
      "\tgrad :  [1. 1.] 5.0 -4.0 -4.0\n",
      "\tgrad :  [2. 4.] 13.0 -22.400001525878906 -44.80000305175781\n",
      "\tgrad :  [3. 9.] 25.0 -28.79999542236328 -86.39998626708984\n",
      "Epoch: 0 | Loss: 23.039993286132812\n",
      "\tgrad :  [1. 1.] 5.0 -0.06400012969970703 -0.06400012969970703\n",
      "\tgrad :  [2. 4.] 13.0 -10.726402282714844 -21.452804565429688\n",
      "\tgrad :  [3. 9.] 25.0 -12.902400970458984 -38.70720291137695\n",
      "Epoch: 1 | Loss: 4.624220848083496\n",
      "\tgrad :  [1. 1.] 5.0 -0.0701436996459961 -0.0701436996459961\n",
      "\tgrad :  [2. 4.] 13.0 -5.037670135498047 -10.075340270996094\n",
      "\tgrad :  [3. 9.] 25.0 -6.0788726806640625 -18.236618041992188\n",
      "Epoch: 2 | Loss: 1.0264637470245361\n",
      "\tgrad :  [1. 1.] 5.0 -0.032088279724121094 -0.032088279724121094\n",
      "\tgrad :  [2. 4.] 13.0 -2.3680877685546875 -4.736175537109375\n",
      "\tgrad :  [3. 9.] 25.0 -2.8571090698242188 -8.571327209472656\n",
      "Epoch: 3 | Loss: 0.22675201296806335\n",
      "\tgrad :  [1. 1.] 5.0 -0.015102386474609375 -0.015102386474609375\n",
      "\tgrad :  [2. 4.] 13.0 -1.113128662109375 -2.22625732421875\n",
      "\tgrad :  [3. 9.] 25.0 -1.3430099487304688 -4.029029846191406\n",
      "Epoch: 4 | Loss: 0.050102103501558304\n",
      "\tgrad :  [1. 1.] 5.0 -0.007098197937011719 -0.007098197937011719\n",
      "\tgrad :  [2. 4.] 13.0 -0.5232353210449219 -1.0464706420898438\n",
      "\tgrad :  [3. 9.] 25.0 -0.6313018798828125 -1.8939056396484375\n",
      "Epoch: 5 | Loss: 0.0110706128180027\n",
      "\tgrad :  [1. 1.] 5.0 -0.0033359527587890625 -0.0033359527587890625\n",
      "\tgrad :  [2. 4.] 13.0 -0.24594879150390625 -0.4918975830078125\n",
      "\tgrad :  [3. 9.] 25.0 -0.29674530029296875 -0.8902359008789062\n",
      "Epoch: 6 | Loss: 0.0024460493586957455\n",
      "\tgrad :  [1. 1.] 5.0 -0.001567840576171875 -0.001567840576171875\n",
      "\tgrad :  [2. 4.] 13.0 -0.11560440063476562 -0.23120880126953125\n",
      "\tgrad :  [3. 9.] 25.0 -0.1394805908203125 -0.4184417724609375\n",
      "Epoch: 7 | Loss: 0.0005404120893217623\n",
      "\tgrad :  [1. 1.] 5.0 -0.0007371902465820312 -0.0007371902465820312\n",
      "\tgrad :  [2. 4.] 13.0 -0.054340362548828125 -0.10868072509765625\n",
      "\tgrad :  [3. 9.] 25.0 -0.06557464599609375 -0.19672393798828125\n",
      "Epoch: 8 | Loss: 0.0001194453943753615\n",
      "\tgrad :  [1. 1.] 5.0 -0.0003452301025390625 -0.0003452301025390625\n",
      "\tgrad :  [2. 4.] 13.0 -0.025543212890625 -0.05108642578125\n",
      "\tgrad :  [3. 9.] 25.0 -0.030818939208984375 -0.09245681762695312\n",
      "Epoch: 9 | Loss: 2.6383528165752068e-05\n",
      "prediction (after training) 4 41.00473403930664\n"
     ]
    }
   ],
   "source": [
    "w1=torch.tensor([1.0],requires_grad=True)\n",
    "w2=torch.tensor([1.0],requires_grad=True)\n",
    "# our model forward pass\n",
    "def forward(x):\n",
    "    return x[0]*w1+x[1]*w2+b\n",
    "\n",
    "#loss function\n",
    "def loss(y_hat,y):\n",
    "    return (y_hat-y)**2\n",
    "\n",
    "#before training\n",
    "print(\"predict (before training)\", 4, forward([4,4**2]).item())\n",
    "\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data.T,y_data):\n",
    "        y_pred = forward(x_val)\n",
    "        l = loss(y_pred, y_val)\n",
    "        l.backward() #Back propagate to update weights\n",
    "        w1.data=w1.data-0.01*w1.grad.item()\n",
    "        w2.data=w1.data-0.01*w2.grad.item()\n",
    "        \n",
    "        print(\"\\tgrad : \",x_val, y_val, w1.grad.item(), w2.grad.item())\n",
    "        \n",
    "        # manually set to zero\n",
    "        w1.grad.data.zero_()\n",
    "        w2.grad.data.zero_()\n",
    "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
    "# after training\n",
    "print(\"prediction (after training)\", 4, forward([4,4**2]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the properties and behaviors of the optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD\n",
    "full-batch가 아닌 mini batch로 학습을 진행하는 방식\n",
    "\n",
    "#### Adagrad\n",
    "학습을 통해 큰 변동이 있는 가중치에 대해서는 학습률을 감소시키고, 아직 가중치의 변동이 별로 없었던 가중치는 학습률을 증가시켜서 학습을 진행하는 방식\n",
    "\n",
    "#### RMSProp\n",
    "Adagrad는 간단한 convex function에서는 잘 동작하지만, 복잡한 다차원 곡면 function에서는 global minimum에 도달하기 전에 학습률이 0으로 수렴할 수 있다. 따라서 RMSProp에서 이를 보완하였다.\n",
    "\n",
    "a) 가중치의 기울기를 단순 누적시키는 것이 아니라, 최신 기울기들이 더 반영되도록 한다.\n",
    "\n",
    "b) hyper parameter p를 추가하여 h가 무한히 커지지 않게 한다.(h는 가중치 기울기 제곱들을 더해가는 term이다.)\n",
    "\n",
    "#### Adam\n",
    "Momentum과 RMSProp을 융합한 방법이다.\n",
    "\n",
    "#### Adamax\n",
    "Adam의 LR을 조절하는 부분의 L2 norm을 Lp norm으로 교체한 방식이다.\n",
    "\n",
    "#### LBFGS\n",
    "LBFGS는 Limited-memory quasi-Newton methods의 한 예시로써, Hessian 행렬을 계산하거나 저장하기 위한 비용이 합리적이지 않을 경우 유용하게 사용된다. 이 방법은 밀도가 높은 n×n의 Hessian 행렬을 저장하는 대신 n차원의 벡터 몇 개만을 유지하여 Hessian 행렬을 추정(approximation)하는 방식이다.\n",
    "\n",
    "#### Rprop\n",
    "Gradient의 부호만을 사용하여 Gradient를 업데이트 하는 방식이다.\n",
    "\n",
    "#### ASGD\n",
    "n 스텝의 가중치를 평균하여 가중치를 업데이트하는 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize pytorch tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch는 autograd 패키지를 통해 자동미분 연산을 지원해줘 역전파 단계의 연산을 자동화 할 수 있다.\n",
    "\n",
    "또한 사용자 정의 모델을 nn.Module을 상속 받아 간단하게 신경망 구성히 가능하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
